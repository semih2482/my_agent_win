# agent/models/llm.py

import os
import sys
import requests
import numpy as np
from tqdm import tqdm
from llama_cpp import Llama
from sentence_transformers import SentenceTransformer
from agent import config
from agent.config import Colors
import threading
import multiprocessing
from huggingface_hub import hf_hub_download


LLM_GPU = None  # Ana model (GPU Ã¼zerinde Ã§alÄ±ÅŸacak)
LLM_CPU = None  # HÄ±zlÄ± model (CPU Ã¼zerinde Ã§alÄ±ÅŸacak)


# Paralel thread'lerin aynÄ± anda GPU'ya yÃ¼klenmesini Ã¶nler, CUDA hatalarÄ±nÄ± engeller.
gpu_lock = threading.Lock()

def _get_cuda_supported_gpu_indices() -> list[int]:
    """Sistemdeki CUDA destekli NVIDIA GPU'larÄ±n indekslerini dÃ¶ndÃ¼rÃ¼r."""
    try:
        import torch
        if not torch.cuda.is_available():
            print(f"{Colors.WARNING}UyarÄ±: PyTorch iÃ§in CUDA mevcut deÄŸil. GPU kullanÄ±mÄ± devre dÄ±ÅŸÄ±.{Colors.ENDC}")
            return []

        device_count = torch.cuda.device_count()
        cuda_devices = []
        for i in range(device_count):
            device_name = torch.cuda.get_device_name(i)
            # Intel, AMD gibi entegre veya harici CUDA olmayan GPU'larÄ± filtrele
            if "nvidia" in device_name.lower():
                cuda_devices.append(i)
                print(f"{Colors.OKGREEN}âœ… CUDA Destekli GPU bulundu: [{i}] {device_name}{Colors.ENDC}")
            else:
                print(f"{Colors.WARNING}âš ï¸ Uyumsuz GPU bulundu ve atlanÄ±yor: [{i}] {device_name}{Colors.ENDC}")
        return cuda_devices
    except ImportError:
        print(f"{Colors.WARNING}UyarÄ±: PyTorch kurulu deÄŸil. GPU tespiti yapÄ±lamÄ±yor.{Colors.ENDC}")
        return []

_embed_model = None
_EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
_EMBED_DIM = 384

def _download_single_model(repo_id: str, filename: str, model_path: str):
    """
    Belirtilen modeli huggingface_hub kullanarak indirir.
    Kimlik doÄŸrulamasÄ± gerektiren (gated) modelleri destekler.
    """
    model_dir = os.path.dirname(model_path)
    os.makedirs(model_dir, exist_ok=True)

    # EÄŸer dosya zaten varsa, indirmeyi atla.
    if os.path.exists(model_path):
        print(f"âœ… Model zaten mevcut: {os.path.basename(model_path)}.")
        return

    print(f"Model indiriliyor: {filename} (repo: {repo_id})")
    try:
        # hf_hub_download, kaydedilmiÅŸ token'Ä± otomatik olarak kullanÄ±r.
        # DosyayÄ± doÄŸrudan projemizdeki 'models' klasÃ¶rÃ¼ne indirir.
        hf_hub_download(
            repo_id=repo_id,
            filename=filename,
            local_dir=model_dir,
            local_dir_use_symlinks=False,  # DosyayÄ± kopyala, sembolik link oluÅŸturma
            resume_download=True,
        )
        print(f"âœ… Model baÅŸarÄ±yla indirildi: {model_path}")

    except Exception as e:
        error_message = str(e)
        if "401" in error_message or "Gated" in error_message:
            print(f"\nâŒ Model indirilemedi: {filename}. Hata: Yetkilendirme HatasÄ± (401).")
            print("LÃ¼tfen Hugging Face web sitesinden model lisansÄ±nÄ± kabul ettiÄŸinizden ve 'huggingface-cli login' ile giriÅŸ yaptÄ±ÄŸÄ±nÄ±zdan emin olun.")
        else:
            print(f"\nâŒ Model indirilemedi: {filename}. Hata: {e}")

        # Ä°ndirme baÅŸarÄ±sÄ±z olduysa kÄ±smi dosyayÄ± temizle
        if os.path.exists(model_path):
            os.remove(model_path)
        raise RuntimeError(f"Model indirilemedi: {filename}") from e

def download_model():
    """YapÄ±landÄ±rmada belirtilen her iki modeli de indirir."""
    # Ana GPU modelini indir
    gpu_model_path = os.path.join(config.PROJECT_ROOT, "models", config.MODEL_FILENAME)
    _download_single_model(config.MODEL_REPO_ID, config.MODEL_FILENAME, gpu_model_path)

    # HÄ±zlÄ± CPU modelini indir
    cpu_model_path = os.path.join(config.PROJECT_ROOT, "models", config.CPU_MODEL_FILENAME)
    _download_single_model(config.CPU_MODEL_REPO_ID, config.CPU_MODEL_FILENAME, cpu_model_path)

def load_model():
    """Hem GPU hem de CPU modellerini belleÄŸe yÃ¼kler."""
    global LLM_GPU, LLM_CPU

    default_threads = max(1, multiprocessing.cpu_count() // 2)

    # GPU modelini yÃ¼kle
    gpu_model_path = os.path.join(config.PROJECT_ROOT, "models", config.MODEL_FILENAME)
    if os.path.exists(gpu_model_path):
        if LLM_GPU is None:
            print("Ana model (GPU) yÃ¼kleniyor...")
            cuda_devices = _get_cuda_supported_gpu_indices()

            # Ã‡oklu GPU veya tek GPU iÃ§in parametreleri ayarla
            llama_params = {
                "model_path": gpu_model_path,
                "n_ctx": config.GPU_N_CTX,
                "n_batch": config.GPU_N_BATCH,
                "n_threads": getattr(config, 'GPU_N_THREADS', default_threads),
                "verbose": False, # Gereksiz baÅŸlangÄ±Ã§ loglarÄ±nÄ± kapatÄ±r.
                "f16_kv": True    # VRAM kullanÄ±mÄ±nÄ± azaltÄ±r ve hÄ±zÄ± artÄ±rÄ±r.
            }

            # Sadece birden fazla CUDA destekli GPU varsa modeli bÃ¶l
            if hasattr(config, 'GPU_SPLIT_WEIGHTS') and config.GPU_SPLIT_WEIGHTS and len(cuda_devices) > 1:
                print(f"{Colors.OKCYAN}âœ… Ã‡oklu GPU modu aktif. Model {len(cuda_devices)} GPU arasÄ±nda bÃ¶lÃ¼nÃ¼yor...{Colors.ENDC}")
                # llama-cpp-python, tensor_split'i otomatik olarak mevcut GPU'lara daÄŸÄ±tÄ±r.
                # Belirli GPU'larÄ± hedeflemek gerekmez, sadece oranlarÄ± vermek yeterlidir.
                llama_params["tensor_split"] = config.GPU_SPLIT_WEIGHTS[:len(cuda_devices)] # Sadece mevcut GPU sayÄ±sÄ± kadar oran kullan
            elif cuda_devices:
                # Tek bir CUDA GPU'su varsa, tÃ¼m katmanlarÄ± ona yÃ¼kle
                print(f"{Colors.OKCYAN}âœ… Tek GPU modu aktif. Model GPU {cuda_devices[0]}'a yÃ¼kleniyor...{Colors.ENDC}")
                llama_params["n_gpu_layers"] = -1 # -1 tÃ¼m katmanlarÄ± yÃ¼kle demektir
            else:
                # HiÃ§ CUDA GPU'su yoksa, CPU'da Ã§alÄ±ÅŸtÄ±r
                print(f"{Colors.WARNING}âš ï¸ CUDA destekli GPU bulunamadÄ±. Model CPU Ã¼zerinde Ã§alÄ±ÅŸacak.{Colors.ENDC}")
                llama_params["n_gpu_layers"] = 0

            LLM_GPU = Llama(**llama_params)
            print(f"âœ… Ana model baÅŸarÄ±yla yÃ¼klendi.")
        else:
            print("â„¹ï¸ Ana model (GPU) zaten yÃ¼klÃ¼.")
    else:
        print("âŒ Ana model (GPU) dosyasÄ± bulunamadÄ±. LÃ¼tfen Ã¶nce indirin.")

    # CPU modelini yÃ¼kle
    cpu_model_path = os.path.join(config.PROJECT_ROOT, "models", config.CPU_MODEL_FILENAME)
    if os.path.exists(cpu_model_path):
        if LLM_CPU is None:
            print("HÄ±zlÄ± model (CPU) yÃ¼kleniyor...")
            LLM_CPU = Llama(
                model_path=cpu_model_path,
                n_ctx=config.CPU_N_CTX,
                n_gpu_layers=0,
                n_batch=config.CPU_N_BATCH, # Config'den gelen deÄŸeri kullan
                n_threads=getattr(config, 'CPU_N_THREADS', default_threads),
                verbose=False
            )
            print("âœ… HÄ±zlÄ± model (CPU) baÅŸarÄ±yla yÃ¼klendi.")
        else:
            print("â„¹ï¸ HÄ±zlÄ± model (CPU) zaten yÃ¼klÃ¼.")
    else:
        print("âŒ HÄ±zlÄ± model (CPU) dosyasÄ± bulunamadÄ±. LÃ¼tfen Ã¶nce indirin.")

def ask(prompt: str, max_new_tokens: int = 256) -> str:
    """Ana GPU modeline bir prompt gÃ¶nderir ve yanÄ±t alÄ±r."""
    global LLM_GPU
    if LLM_GPU is None:
        raise RuntimeError("Ana model (GPU) yÃ¼klenmedi. load_model() Ã§aÄŸÄ±rÄ±n!")

    with gpu_lock:
        try:
            # Ã–NEMLÄ°: 'reset=True' eklemek, her Ã§aÄŸrÄ±nÄ±n
            # taze bir 'n_batch' ayarÄ±yla baÅŸlamasÄ±nÄ± saÄŸlar ve hÄ±zÄ± korur.
            # LLM_GPU.reset()

            response = LLM_GPU.create_completion(
                prompt,
                max_tokens=max_new_tokens,
                temperature=config.GPU_TEMPERATURE,
                top_p=config.GPU_TOP_P,
                repeat_penalty=config.GPU_REPEAT_PENALTY,
            )
            text = response['choices'][0]['text'].strip()
            return text
        except Exception as e:
            return f"[HATA] Ana model (GPU) yanÄ±t veremedi: {e}"

def ask_fast_cpu(prompt: str, max_new_tokens: int = 128) -> str:
    """HÄ±zlÄ± CPU modeline bir prompt gÃ¶nderir ve yanÄ±t alÄ±r."""
    global LLM_CPU
    if LLM_CPU is None:
        raise RuntimeError("HÄ±zlÄ± model (CPU) yÃ¼klenmedi. load_model() Ã§aÄŸÄ±rÄ±n!")

    try:
        # print(">>> HÄ±zlÄ± LLM'e (CPU) soruluyor...")
        response = LLM_CPU.create_completion(
            prompt,
            max_tokens=max_new_tokens,
            temperature=config.CPU_TEMPERATURE,
            top_p=config.CPU_TOP_P,
            repeat_penalty=config.GPU_REPEAT_PENALTY,
        )
        # print("<<< HÄ±zlÄ± LLM'den (CPU) yanÄ±t alÄ±ndÄ±.")
        text = response['choices'][0]['text'].strip()
        return text
    except Exception as e:
        return f"[HATA] HÄ±zlÄ± model (CPU) yanÄ±t veremedi: {e}"



def _load_embed_model(model_name: str = _EMBED_MODEL_NAME):
    global _embed_model
    if _embed_model is None:
        try:
            print("ðŸ”¹ Embedding modeli yÃ¼kleniyor...")
            _embed_model = SentenceTransformer(model_name)
            print("âœ… Embedding modeli yÃ¼klendi.")
        except Exception as e:
            print(f"[embed yÃ¼kleme hatasÄ±] {e}")
            _embed_model = None

def embed(text: str):
    """
    Text'i embedding'e Ã§evirir.
    """
    global _embed_model
    if _embed_model is None:
        _load_embed_model()

    if _embed_model is None:
        raise RuntimeError(
            "Embedding modeli yÃ¼klenemedi. "
            "LÃ¼tfen 'pip install sentence-transformers' komutunu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±ndan "
            "ve internet baÄŸlantÄ±ndan emin ol."
        )

    vec = _embed_model.encode([text], convert_to_numpy=True)[0]
    return vec.astype("float32")
