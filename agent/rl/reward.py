# agent/rl/reward.py

from typing import Optional

class RewardSignal:
    """
    KullanÄ±cÄ± geri bildirimi + metriklerden reward Ã¼retir.
    Feedback, latency, tool error, sentiment ve retry sayÄ±sÄ±nÄ± birleÅŸtirir.
    """

    def __init__(self):
        # Ã–dÃ¼l sinyali aÄŸÄ±rlÄ±klarÄ± (Ã–nem sÄ±rasÄ±na gÃ¶re ayarlanmÄ±ÅŸtÄ±r)
        self.W_FEEDBACK = 2.0  # KullanÄ±cÄ± geri bildirimi en yÃ¼ksek Ã¶ncelik
        self.W_ERROR = 1.5     # HatalarÄ± gÃ¼Ã§lÃ¼ cezalandÄ±r
        self.W_RETRY = 1.0     # VerimsizliÄŸi cezalandÄ±r
        self.W_LATENCY = 0.5   # HÄ±zÄ± teÅŸvik et
        self.W_SENTIMENT = 0.0 # (Åimdilik devre dÄ±ÅŸÄ±)
        pass

    def from_feedback(self, feedback: str) -> float:
        feedback = feedback.lower()
        if feedback in ["yes", "ğŸ‘", "good", "correct"]:
            return 1.0
        elif feedback in ["no", "ğŸ‘", "bad", "wrong"]:
            return -1.0
        return 0.0

    def from_latency(self, start_time: float, end_time: float) -> float:
        latency = end_time - start_time
        # Yeni LLM ayarlarÄ±nla (30-60 saniye sÃ¼rebildiÄŸi iÃ§in) toleransÄ± biraz artÄ±ralÄ±m
        if latency < 5.0:  # 5 saniyeden hÄ±zlÄ±: MÃ¼kemmel
            return 0.5
        elif latency < 15.0: # 15 saniyeden hÄ±zlÄ±: Kabul edilebilir
            return 0.0
        return -0.5

    def from_tool_error(self, error: Optional[str]) -> float:
        """AraÃ§ hatalarÄ±nÄ± cezalandÄ±r"""
        if not error:
            return 0.0
        return -1.0 # Hata varsa daha bÃ¼yÃ¼k ceza ver (Ã¶nceki -0.5 yerine)

    def from_sentiment(self, text: str) -> float:
        """
        Basit pozitif/negatif kelime tabanlÄ± sentiment reward.
        Åu an iÃ§in reward toplamÄ±na dahil edilmemektedir (W_SENTIMENT = 0.0).
        """
        positive_words = ["iyi", "harika", "teÅŸekkÃ¼r", "ğŸ‘", "baÅŸarÄ±lÄ±"]
        negative_words = ["kÃ¶tÃ¼", "hata", "ğŸ‘", "baÅŸarÄ±sÄ±z", "Ã§Ã¶ktÃ¼"]
        reward = 0.0
        for w in positive_words:
            if w in text.lower():
                reward += 0.5
        for w in negative_words:
            if w in text.lower():
                reward -= 0.5
        return reward

    def from_retry(self, retries: int, max_retries: int = 3) -> float:
        """
        Retry sayÄ±sÄ±na gÃ¶re reward azalt.
        0 retry â†’ +1, max retry â†’ -1 civarÄ±.
        """
        if retries <= 0:
            return 1.0
        penalty = min(retries / max_retries, 1.0) # Penalty'nin 1'i geÃ§mesini engelle
        return 1.0 - 2 * penalty # retry arttÄ±kÃ§a reward dÃ¼ÅŸer (max retry = -1.0)

    def total_reward(
        self,
        feedback: str,
        start_time: float,
        end_time: float,
        error: Optional[str] = None,
        user_text: str = "",
        retries: int = 0
    ) -> float:
        """
        TÃ¼m reward sinyallerini aÄŸÄ±rlÄ±klandÄ±rarak birleÅŸtirir ve tek float dÃ¶ndÃ¼rÃ¼r.
        """
        r_feedback = self.from_feedback(feedback)
        r_latency = self.from_latency(start_time, end_time)
        r_error = self.from_tool_error(error)
        r_sentiment = self.from_sentiment(user_text) # Hesapla ama aÄŸÄ±rlÄ±ÄŸÄ± 0
        r_retry = self.from_retry(retries)

        # AÄŸÄ±rlÄ±klandÄ±rÄ±lmÄ±ÅŸ Toplam
        total = (r_feedback * self.W_FEEDBACK) + \
                (r_latency * self.W_LATENCY) + \
                (r_error * self.W_ERROR) + \
                (r_retry * self.W_RETRY) + \
                (r_sentiment * self.W_SENTIMENT) # AÄŸÄ±rlÄ±k 0 olduÄŸu iÃ§in ÅŸimdilik etkisiz.


        return total